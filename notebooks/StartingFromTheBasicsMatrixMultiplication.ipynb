{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10bac973-ff9d-4fb8-880a-b1d480a115b7",
   "metadata": {},
   "source": [
    "# Machine Learning from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbccff5-51b7-463f-a855-e1db9283793f",
   "metadata": {},
   "source": [
    "I decided I wanted to review Machine Learning from the bottom up for some personal projects of my own, I realized my education and path in the field so far were missing some fundementals in ways that have been having some consequences in complex problems. I believe that missing out in how the absolute basics are being computed have caused me to make some mistakes that would have been easily avoided had I implemented some of the underlying algorithms from scratch. While I did implement the true \"basics\" such as Matmuls in undergrad, I never truly covered back propagation at the implementation level (unless you count my very easy vector calc course, which I don't). These resources will seek to build the fundemental ideas of Machine Learning from the ground up while attempting to explain the math at every level. The math involved in the derivation fo these algorithms will be the key focus of these posts, and the code's sole purpose will be to explain the math. This is because during my time reviewing many resources online, I found that a lot of derivations skip some very crucial steps in understanding why many simplifications work, which can lead to erroneous assumptions in how a library might make many computations. The most powerful tools libraries like Torch have, are the autograd systems. However, without a deep understanding of the underlying math, one can easily go down many wrong paths. It is also very easy to dupe oneself into thinking that because they covered the math basics at an introductory level in undergrad, they have a clear picture of what is going on underneath the hood. While it is true that a lot of the math being used is very basic, when applied in the context of Machine Learning, a lot of the finer details can be overlooked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d513f-db68-4ad4-9980-e6d458ffff6e",
   "metadata": {},
   "source": [
    "## The Dot Product and Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca0dbd-b602-4ace-9b5a-d4c1ab1db49d",
   "metadata": {},
   "source": [
    "This doesn't require very much review, and I'll assume anyone reading this understands what \"vectors\" are, if you don't as a review I will remind you that a vector is just an element of a vector space. Which is a mathematicians way of saying, it's just any element that exists within a larger space of elements. In our case it's numbers which are used to represent a linear function $x_1 + x_2 +x_3 + \\cdots + x_n$. In Machine Learning we only ever work with the Real numbers represented as $\\mathbb{R}$. Libraries like torch do have some operations that do end up using the Complex numbers for things like computing EigenValues, but that is a bridge to be crossed when we get there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20812551-3d33-4dbf-9fab-7a114f832e7a",
   "metadata": {},
   "source": [
    "## The dot product\n",
    "The dot product is a simple operation, it takes two vectors of length $n$ and it multiplies them together into a single scaler value. If you don't remember what a scaler is, think of it as a constant that can be used to multiply a vector, affecting its magnitude if it's positive and direction if it's negative. It can be described as such: $$a = a_1 + a_2 + a_3 + \\cdots + a_n $$ $$b= b_1 + b_2 + b_3 \\cdots + b_n$$ $$a \\bullet b = a_1 b_1 + a_2 b_2 + a_3 b_3 \\cdots a_n b_n $$ $$a \\bullet b = \\sum_{i=1}^n a_i * b_i$$\n",
    "\n",
    "Python Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a497f8-c3fa-4fa3-a419-b3b3168b0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(a,b):\n",
    "    assert len(a) == len(b)\n",
    "    res = 0\n",
    "    for ai,bi in zip(a,b):\n",
    "        res += ai * bi\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc83bc-68fd-41a4-bc08-990e9201eca5",
   "metadata": {},
   "source": [
    "Throughout these notebooks we will also be periodically comparing our implementations to those of many libraries to verify correctness, here is a demonstration using the numpy dot to confirm this is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0be75cb-dd65-4780-b82c-107a760af1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [.4,.3,.2,.6]\n",
    "b = [.1,.5,.6,.9]\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd65ac0-8340-4214-be50-865653b112d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500000000000001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc1ca682-ee5f-4b04-bdfe-966af9daeda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500000000000001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317bdc41-0a17-4520-b723-87a1eed9ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.dot(a,b) == dot_product(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f8991f-923f-42f2-b061-d16cae0421c8",
   "metadata": {},
   "source": [
    "Now that we've demonstrated the math and the code for the dot product we will be using the numpy implementation going forward, this is because it's optimized, part of the purpose of this notebook is to build it from the ground up so we can understand the libraries we regularly rely on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770acbe0-5ecf-4a32-a449-080269d83e17",
   "metadata": {},
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bdde15-3d2a-4349-b234-cc7b028e942e",
   "metadata": {},
   "source": [
    "The next step is matrix multiplication. Suppose we have two matrices, a and b, a matmul b is a linear map that produces c, represented as such: $$\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix} \\bullet \\begin{bmatrix}\n",
    "b_{11} & b_{12} & b_{13}\\\\\n",
    "b_{21} & b_{22} & b_{23}\\\\\n",
    "b_{31} & b_{32} & b_{33}\n",
    "\\end{bmatrix}  =\\begin{bmatrix} a_{11} b_{11} + a_{12} b_{21} + a_{13} b_{31} &  a_{11} b_{12} + a_{12} b_{22} + a_{13} b_{32} &  a_{11} b_{13} + a_{12} b_{23} + a_{13} b_{33} \\\\\n",
    "a_{21} b_{11} + a_{22} b_{21} + a_{23} b_{31} & a_{21} b_{12} + a_{22} b_{22} + a_{23} b_{32}& a_{21} b_{13} + a_{22} b_{23} + a_{23} b_{33}\\\\\n",
    "a_{31} b_{11} + a_{32} b_{21} + a_{33} b_{31} & a_{31} b_{12} + a_{32} b_{22} + a_{33} b_{32} & a_{31} b_{13} + a_{32} b_{23} + a_{33} b_{33}\n",
    "\\end{bmatrix}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3b865-ee53-46d9-946d-f53db4703bfd",
   "metadata": {},
   "source": [
    "Viewing this, it can be deduced that a matmul is a linear map that takes the dot product of each row in matrix A with each column in matrix B producing an element in matrix C. This means matrix multiplication posseses some very important mathematical features: $$additivity : f(u +v) = f(u) + f(v)$$ and $$homogeneity : f(c * v) = c * f(v) $$ Note: here c is a scaler multiple. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec602bbf-f326-4fef-a69f-7033aafd9473",
   "metadata": {},
   "source": [
    "These properties are very important because they will allow us to simplify our derivations in many places, reducing compute costs. Matrix multiplications tend to hover around $O(n^3)$ complexity, meaning that any algorithm that implements it must make ~$N^3$ computer operations, so allowing us to apply addition and scaler multiplication to them $\\textit{after}$ the matmul or even other functions that have the properties of linear maps reduces an insane amount of overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbf81c6-82ac-4675-86e0-8c9c73fef3b6",
   "metadata": {},
   "source": [
    "### Some consequences of the Matrix Multiplication\n",
    "\n",
    "Before diving into the implementation, I'd like to point out that so far we've only actually covered the base cases of matrix multiplication, that is when two matrixes have the same dimensions. The dot product is really just the case where both matrices have a single column. However it is possible to multiply two matrices with slightly different shapes and this is incredibly important, as it is the operation that is used in machine learning to change the dimensions of input features within neural networks, but I am getting ahead of myself. The main thing understand is that if A is a matrix with dimensions: $$A = A_i , A_j$$ and B is a matrix: $$B = B_k , B_l$$ the two matrices can only be multiplied if and only if $$j =k $$\n",
    "and the resulting matrix will have the dimensions $$C = C_i, C_l$$\n",
    "\n",
    "#### Non Commutativity and the Determinant property\n",
    "\n",
    "The last important properties I will mention here is that even though a Matrix Multiplication is a Linear Map, Matrix Multiplication does not have the Commutative property! That means: $$A\\bullet B \\neq B \\bullet A$$ This is extremely important to remember throughout machine learning and even when relying on things like torch autograd, because it means that many operations need to maintain a consistent order to function. Many mistakes and errors can be traced back to simple mistakes involving the order of an operation. The determinant property which we don't need right now simply states that $det(AB) = det(A) det(B)$. I'm mentioning this seperately because even though we covered linear maps earlier, the determinant is actually a multilinear map. We won't be using this property for a while, but it is important to keep in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be3999-308a-4b8f-aa79-8c2c7b5d740d",
   "metadata": {},
   "source": [
    "#### The Transpose of a Matrix\n",
    "The Transpose is a Linear Map operation that can be applied to a matrix. It is defined as interchanging a matrix's rows and columns. Here is an example:\n",
    " $$A= \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}$$ then $$A^T = \\begin{bmatrix}\n",
    "a_{11} & a_{21} & a_{31}\\\\\n",
    "a_{12} & a_{22} & a_{32}\\\\\n",
    "a_{13} & a_{23} & a_{33}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4b5b8a-dedf-462c-91cc-14f3c2cfd9ac",
   "metadata": {},
   "source": [
    "##### Implementing Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bcd1c3f-7cc6-4f4b-8cbe-25de7618fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column(matrix,i):\n",
    "    j = 0\n",
    "    col = []\n",
    "    while j < len(matrix):\n",
    "        col.append(matrix[j][i])\n",
    "        j += 1\n",
    "    return col\n",
    "def get_row(matrix,i):\n",
    "    j = 0\n",
    "    row = []\n",
    "    while j < len(matrix[0]):\n",
    "        row.append(matrix[i][j])\n",
    "        j += 1\n",
    "    return row\n",
    "def matmul(a,b):\n",
    "    a_rows,a_cols = len(a), len(a[0])\n",
    "    b_rows,b_cols = len(b),len(b[0])\n",
    "    assert a_cols == b_rows\n",
    "    C = [[0 for i in range(a_rows)] for j in range(b_cols)]\n",
    "    for i in range(len(C)):\n",
    "        for j in range(len(C[0])):\n",
    "            arow = get_row(a,i)\n",
    "            bcol = get_column(b,j)\n",
    "            v = 0\n",
    "            for k,l in zip(arow,bcol):\n",
    "                v += k * l\n",
    "            C[i][j] = v\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b1ae402-ddfa-471c-8f78-5e54d895a82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[711, 635, 1158], [1025, 918, 1645], [1195, 1117, 2165]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,3,7],[3,4,4,10],[7,9,9,10]]\n",
    "b = [[5,6,5],[7,8,8],[23,27,92],[89,76,123]]\n",
    "matmul(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c653d899-7172-43f1-8d09-4bd2038316de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 711,  635, 1158],\n",
       "       [1025,  918, 1645],\n",
       "       [1195, 1117, 2165]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bdcd9e2-25d0-4ba3-add1-aafdda0d8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.array(matmul(a,b)) == np.matmul(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82a802-e09c-44e0-8188-b0c6fc5fddc2",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Now that we have set up the very basics we can begin implementing linear regression and build from there, in the next section we will use these topics to derive basic linear regression, and from there we can build up to the Multi Layer Perceptron, the heart of Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782ead7-37fa-47a0-8425-b489c05a70a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python alea-jupyter",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
