{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6861e2f9-e981-48f8-b67a-768fb530551a",
   "metadata": {},
   "source": [
    "# The Binary Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ea4ef-b682-45ed-97cf-ffe9eb01e3f0",
   "metadata": {},
   "source": [
    "So far I've covered Neural Networks in the context of regression, i.e. approximating continous functions. However in the real world many use cases require approximating functions with discrete values. Classic examples of this are things like image detection, such as \"does a patient have breast cancer or not?\" To give a better example of this, I will introduce a dataset that we can examine prior to getting into the details of how to tackle this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee280a8a-45ed-4989-a9c1-453731f14a0f",
   "metadata": {},
   "source": [
    "## The Breast Cancer dataset\n",
    "To work with this we will need to use sklearn's breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e1253-3ab3-434d-bdc7-e15b041e1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "# Load the breast_cancer dataset\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5bd9c-2660-42e3-8653-de25f878244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98861f8-3818-42f0-b5ed-8f64be7e8dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dcfb61-e710-49ff-96ad-abddb82a427f",
   "metadata": {},
   "source": [
    "We can see here that the dataset's feature values are continuous and the labels are discrete. So how can we approximate the labels using the features? The first step is luckily something I've covered before. The sigmoid function, recall that it has the convenient property of clamping any value in $\\mathbb{R}$ between the interval [0,1]. That means that by defining a neural network similarly to how we did under Linear Regression but applying the sigmoid function as an activation to the final layer, the neural network can be treated as a function that estimates the probability that a feature vecttor corresponds to a 1. In this particular case 1 represents the boolean answer to the question \"Does the patient have breast cancer?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a44d117-ebe0-46bc-9614-7f4ad54dd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(x) / ( 1 + np.exp(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f03b0-b24a-47be-8d9a-e7384e28fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "# graphing the sigmoid function\n",
    "fa = np.linspace(-10, 10, 100) \n",
    "\n",
    "\n",
    "a = sigmoid(fa)\n",
    "plt.plot(fa, a) \n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"sigmoid(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e53fe-1d1f-4eb5-bb10-94ccdd01434a",
   "metadata": {},
   "source": [
    "## The loss function in binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cceecd-5a31-44a7-aa18-bb5a6f390d7c",
   "metadata": {},
   "source": [
    "The probability estimation given by the output of the neural network's forward pass is nice, but how do we optimize the network? How do we perform gradient descent on the network if it's giving probabilities, the MSE loss won't work because the loss will always be tiny. At most the loss will only ever be 1, and if the neural consistently guesses .5 the loss function will be .25 for values of both 0 and 1. The main solution to this problem is the Binary Cross Entropy Loss Function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9651cb8-72e4-476a-9d9c-bc2b0e014a9f",
   "metadata": {},
   "source": [
    "### The Binary Cross Entropy Loss Function\n",
    "The binary cross entropy loss can be defined as $$L = - [y \\cdot \\ln (\\hat{y}) + (1 - y) \\cdot \\ln( 1 - \\hat{y})]$$\n",
    "Where $ln$ refers to the natural log and $\\hat{y}$ refers to the output of the neural network with the sigmoid function applied to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc16c78-1c65-46ef-82ad-5b6719b65c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphing the natural log function\n",
    "fa = np.linspace(1e-4, 10, 100) \n",
    "\n",
    "\n",
    "a = np.log(fa)\n",
    "plt.plot(fa, a) \n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"natual log(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86cbb20-2ca8-4c6e-9f80-9d0c99302271",
   "metadata": {},
   "source": [
    "The cross entropy loss gives a much better identification of how \"off\" a model from the target, while also being continuous, when $$y = 1,\\hat{y} = .5, L = .693$$\n",
    "$$y = 0,\\hat{y} = .5, L = .693$$\n",
    "\n",
    "The values are still small at $.5$ but it is at least a higher penalty than $.25$ with MSE, furthermore the true value of the Cross Entropy loss lies closer to 0 and one when $$y = 1, \\hat{y} = .1 , L = 2.3$$ $$y = 0,\\hat{y} = .1 , L =.10$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1c084-696d-454b-8d13-30d27d298e3d",
   "metadata": {},
   "source": [
    "### Graphing the difference between MSE and Binary Cross entropy\n",
    "\n",
    "To get a better understanding of what is going on, I've provided a graph showing how the Binary Cross Entropy Loss function looks vs the MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58180f00-605a-4208-b237-91e165632d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y,y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + ( 1 - y) * np.log(1 - y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0bd24-de75-4f4d-b57d-ce16f7004d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(targets,outputs):\n",
    "    return np.mean((targets - outputs) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e538374-b711-4d2b-8003-4f8660ce2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "\n",
    "y = np.random.randint(0, 2, size=num_samples)\n",
    "\n",
    "\n",
    "raw_predictions = np.random.uniform(-10, 10, size=num_samples)\n",
    "\n",
    "y_hat = sigmoid(raw_predictions)\n",
    "\n",
    "\n",
    "loss = - (y * np.log(y_hat) + ( 1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(y_hat, loss, alpha=0.5, c=y, cmap='bwr', edgecolors='k', label='Loss per sample')\n",
    "plt.xlabel(\"Predicted Probability (y_hat)\") \n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Cross-Entropy Loss vs. Predicted Probability\")\n",
    "plt.colorbar(scatter, label='True Label (y)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "average_loss = cross_entropy_loss(y, y_hat) / num_samples\n",
    "print(f\"Average Cross-Entropy Loss: {average_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f44e3-0bdd-4b41-8730-024f2fe75e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "\n",
    "y = np.random.randint(0, 2, size=num_samples)\n",
    "\n",
    "\n",
    "raw_predictions = np.random.uniform(-10, 10, size=num_samples)\n",
    "\n",
    "y_hat = sigmoid(raw_predictions)\n",
    "\n",
    "\n",
    "loss = (y - y_hat)**2\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(y_hat, loss, alpha=0.5, c=y, cmap='bwr', edgecolors='k', label='Loss per sample')\n",
    "plt.xlabel(\"Predicted Probability (y_hat)\") \n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"MSE Loss vs. Predicted Probability\")\n",
    "plt.colorbar(scatter, label='True Label (y)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "average_loss = mse_loss(y, y_hat) / num_samples\n",
    "print(f\"Average MSE Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3757216-62f7-425b-95f8-5bcdb7236392",
   "metadata": {},
   "source": [
    "## Gradient Descent using the Cross Entropy Loss Function using Logistic Regression\n",
    "In order to get a good understanding of how a model might work, I will begin with the most basic model. Logistic Regression, that is, a model defined as $$\\hat{y} =\\sigma( x_1 w_1 + x_2 w_2 + x_3 w_3 + ...+ x_n w_n + b) $$ $$\\hat{y} = \\sigma(X \\cdot W + b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674bf73-4b4c-43e9-8b9d-b3909eb53fc0",
   "metadata": {},
   "source": [
    "$$L = - [y \\cdot \\ln (\\hat{y}) + (1 - y) \\cdot \\ln( 1 - \\hat{y})]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34fc30e-11b5-48c9-97c2-a2f0f502b3a3",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L }{\\partial \\hat{y}} = -y \\cdot \\frac{1}{\\hat{y}}  + - [(1 - y) \\cdot \\frac{-1}{1 - \\hat{y}}]$$\n",
    "$$\\frac{\\partial L }{\\partial \\hat{y}} =  \\frac{-y}{\\hat{y}}  +  \\frac{ 1 - y}{1 - \\hat{y}}$$\n",
    "$$\\frac{\\partial L }{\\partial \\hat{y}} =  \\frac{-y (1 - \\hat{y})}{(1 - \\hat{y})\\hat{y}}  +  \\frac{\\hat{y}  (1 - y)}{\\hat{y} (1 - \\hat{y})}$$\n",
    "$$\\frac{\\partial L }{\\partial \\hat{y}} = \\frac{-y + y\\hat{y} + \\hat{y} - y \\hat{y}}{\\hat{y} - \\hat{y}\\hat{y}}$$\n",
    "$$\\frac{\\partial L }{\\partial \\hat{y}} = \\frac{\\hat{y} - y}{ \\hat{y} - \\hat{y} \\hat{y}}$$\n",
    "$$\\frac{\\partial L }{\\partial \\hat{y}} = \\frac{\\hat{y} - y}{ \\hat{y} ( 1 - \\hat{y}) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57831d-44ca-4a7f-aaff-eb1508ed72a1",
   "metadata": {},
   "source": [
    "To find the gradient of a given $w_i$ we now need $$\\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w_i}$$\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial w_i} = \\frac{\\partial \\sigma(X \\cdot W + b) }{\\partial w_i} \\cdot \\frac{\\partial (X \\cdot W + b)}{\\partial w_i}$$\n",
    "$$\\frac{\\partial \\sigma(X \\cdot W) }{\\partial w_i} = \\sigma(X \\cdot W + b) \\cdot (1 - \\sigma(X \\cdot W + b))$$\n",
    "$$\\frac{\\partial X \\cdot W}{\\partial w_i} = x_i$$\n",
    "$$\\frac{\\partial L}{\\partial w_i} = \\frac{\\hat{y} - y}{ \\hat{y} ( 1 - \\hat{y}) } \\cdot \\sigma(X \\cdot W + b) \\cdot (1 - \\sigma(X \\cdot W + b)) \\cdot x_i $$\n",
    "because $\\sigma(X \\cdot W + b)  = \\hat{y}$ \n",
    "$$\\frac{\\partial L}{\\partial w_i} = \\frac{\\hat{y} - y}{ \\hat{y} ( 1 - \\hat{y}) } \\cdot \\hat{ y}\\cdot (1 - \\hat{y}) \\cdot x_i $$\n",
    "$$\\frac{\\partial L}{\\partial w_i} = (\\hat{y} - y ) \\cdot x_i$$\n",
    "$$ \\frac{\\partial L}{\\partial W} = (\\hat{y} - y ) \\cdot X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a6fe8-5bf4-4119-9a4c-b1d197ad9658",
   "metadata": {},
   "source": [
    "## BackPropagation with Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9340d65-1006-47e0-846a-80f6dda8aa26",
   "metadata": {},
   "source": [
    "This however only describes the most basic model, we however want more complicated neural networks, so we will describe a network\n",
    "$$\\hat{y} = \\sigma( \\sigma(\\sigma(x \\cdot w_1 + b_1) \\cdot w_2 + b_2) \\cdot w_3 + b_3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1801dc-b1f2-4bd9-8070-a1dc28cc3297",
   "metadata": {},
   "source": [
    "The process works fairly similarly to how it did in linear regression. The key difference is the output activation that we have to worry about, so we will begin by deriving $\\nabla W_3$ we need $$\\frac{\\partial L}{\\partial W_3} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial W_3}$$\n",
    "$$\\frac{\\partial L }{\\partial \\hat{y}} = \\frac{\\hat{y} - y}{ \\hat{y} ( 1 - \\hat{y}) }$$\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial W_3} =  \\sigma( \\sigma(\\sigma(x \\cdot w_1 + b_1) \\cdot w_2 + b_2) \\cdot w_3 + b_3) \\cdot (1 -  \\sigma( \\sigma(\\sigma(x \\cdot w_1 + b_1) \\cdot w_2 + b_2) \\cdot w_3 + b_3)) \\cdot \\sigma(\\sigma(x \\cdot w_1 + b_1) \\cdot w_2 + b_2)$$\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial W_3} = (\\hat{y} \\cdot (1 - \\hat{y})) \\cdot \\sigma(\\sigma(x \\cdot w_1 + b_1) \\cdot w_2 + b_2)$$\n",
    "$$\\frac{\\partial L }{\\partial W_3} = \\frac{\\hat{y} - y}{ \\hat{y} ( 1 - \\hat{y}) } \\cdot (\\hat{y} \\cdot (1 - \\hat{y})) \\cdot \\sigma(\\sigma(x \\cdot w_1 + b_1) \\cdot w_2 + b_2)$$\n",
    "$$ \\frac{\\partial L }{\\partial W_3 } = (\\hat{y} - y) \\cdot \\sigma(\\sigma(x \\cdot w_1 + b_1) \\cdot w_2 + b_2)$$\n",
    "this can be written more formally as $$\\frac{\\partial L }{\\partial W_3 } = (\\hat{y} - y) \\cdot A_2$$ where $A_2$ is the output of the second layer with the sigmoid activation applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a158ea-9289-4b3e-b774-a264be0ba156",
   "metadata": {},
   "source": [
    "Now we can describe $\\nabla W_2$\n",
    "$$\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial W_2}$$\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial W_2} = A_1 \\cdot \\hat{y} ( 1 - \\hat{y}) \\cdot W_3   \\cdot \\sigma(Z_2) \\odot\\sigma( 1- Z_2)$$\n",
    "where $Z_2$ is the output of the second layer without the activation\n",
    "thus $$\\frac{\\partial L}{\\partial W_2} = A_1 \\cdot (\\hat{y} - y ) \\cdot W_3 \\cdot \\sigma(Z_2) \\odot\\sigma( 1- Z_2) $$\n",
    "$\\odot $ is element wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5bf4b8-8a59-4041-92ed-e0b00a88c69f",
   "metadata": {},
   "source": [
    "Notice that this simplifies ot the general algorithm we defined in the standard regression Backpropagation of $$\\nabla W_i = A(O)_{i-1}^T [\\frac{\\partial L}{\\partial O_{i+1}} \\bullet W_{i + 1}^T \\odot A^{\\prime}(O_i)]$$\n",
    "\n",
    "$$\n",
    "A(x) =\n",
    "\\begin{cases}\n",
    "x, & \\text{if } i = 0 \\text{ (input layer)} \\\n",
    "\\sigma(x), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93ac58-2de8-429c-ac2e-8b344fcee773",
   "metadata": {},
   "source": [
    "## Implementing a Binary Cross Entropy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1124eea-f690-4a1f-bc8c-2440febd2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "# Load the breast_cancer dataset\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "X,y = X.to_numpy(),y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07e52d-d24e-4429-9ee2-7ac7ed71c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return stable_sigmoid(x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a63499-d545-4c26-810c-5034913a7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for a single Layer\n",
    "class Layer(object):\n",
    "    def __init__(self,input_dimensions,output_dimensions):\n",
    "        self.weights = np.random.randn(input_dimensions,output_dimensions)\n",
    "        self.bias = np.random.randn(output_dimensions) \n",
    "        self.output_dimensions = output_dimensions\n",
    "    def forward(self,x):\n",
    "        return np.matmul(x,self.weights) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0a2c6-2604-4899-b879-5519404cc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(object):\n",
    "    def __init__(self,input_dimensions,hidden_layers = [64,128], activation = sigmoid, activation_derivative = sigmoid_derivative):\n",
    "        self.layers =[Layer(input_dimensions,hidden_layers[0])]\n",
    "        #Create the layers\n",
    "        for i in range(1 , len(hidden_layers) ):\n",
    "            self.layers.append(Layer(self.layers[-1].output_dimensions,hidden_layers[i]))\n",
    "        # Create the output Layer\n",
    "        self.output_layer = Layer(self.layers[-1].output_dimensions,1)\n",
    "        #Activation function\n",
    "        self.activation = activation\n",
    "        #derivative of the activation function\n",
    "        self.activation_derivative = activation_derivative\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer.forward(x))\n",
    "        return self.output_layer.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71147f-4923-440d-8a9e-b184869f21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return stable_sigmoid(x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2fa2b0-df17-429b-982c-e14890dfd5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = MultiLayerPerceptron(30)\n",
    "sigmoid(neural_network.forward(X)).shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da20b10-bfb0-4b7f-8bca-eeee73da3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(neural_network,features,targets,learning_rate = 0.0001):\n",
    "    targets = np.reshape(targets,(targets.shape[0], 1))\n",
    "    activations = []\n",
    "    inputs = []\n",
    "    inputs.append(features)\n",
    "    activations.append(features)\n",
    "    current_input = features\n",
    "    current_activation = features\n",
    "    for layer in neural_network.layers:\n",
    "        current_input = np.matmul(current_activation,layer.weights) + layer.bias\n",
    "        current_activation = neural_network.activation(current_input)\n",
    "        inputs.append(current_input)\n",
    "        activations.append(current_activation)\n",
    "    current_input = np.matmul(current_activation, neural_network.output_layer.weights) + neural_network.output_layer.bias\n",
    "    current_activation = sigmoid(current_input)\n",
    "\n",
    "    loss =  cross_entropy_loss(targets,current_activation)\n",
    "    error = current_activation - targets\n",
    "    \n",
    "    current_input = inputs.pop()\n",
    "    current_activation = activations.pop()\n",
    "    \n",
    "    final_layer_gradient =  np.dot(current_activation.T, error) \n",
    "    \n",
    "    offset = 1 / len(targets)\n",
    "\n",
    "    neural_network.output_layer.weights -= offset * learning_rate * final_layer_gradient\n",
    "    neural_network.output_layer.bias -= offset * learning_rate * np.sum(error,axis = 0)\n",
    "\n",
    "    current_derivative = np.dot(error,neural_network.output_layer.weights.T) * neural_network.activation_derivative(current_input)\n",
    "    current_input = inputs.pop()\n",
    "    current_activation = activations.pop()\n",
    "    current_gradient = np.dot(current_activation.T,current_derivative)\n",
    "    neural_network.layers[-1].weights -= offset * learning_rate * current_gradient\n",
    "    neural_network.layers[-1].bias -= offset *learning_rate * np.sum(current_derivative,axis=0)\n",
    "\n",
    "    for i in reversed(range(len(neural_network.layers) - 1)):\n",
    "        current_derivative = np.dot(current_derivative,neural_network.layers[i  + 1 ].weights.T) * neural_network.activation_derivative(current_input)\n",
    "        current_input = inputs.pop()\n",
    "        current_activation = activations.pop()\n",
    "        current_gradient = np.dot(current_activation.T,current_derivative)\n",
    "        neural_network.layers[i].weights -= offset * learning_rate * current_gradient\n",
    "        neural_network.layers[i].bias -= offset * learning_rate * np.sum(current_derivative,axis=0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6580a2-0ac3-4edf-a78c-340fe388f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(neural_network,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653977a3-849e-4f0d-afc7-ee411a22e62b",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bddca-367b-42ed-8994-78f9fff3d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(X)\n",
    "permutation = np.random.permutation(num_samples)\n",
    "shuffled_X = X[permutation]\n",
    "shuffled_y = y[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9fd5d-1ba3-41a7-9b75-9a9bdac88eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y = shuffled_X[0:len(X) - 100],shuffled_y[0:len(y) - 100]\n",
    "test_X, test_y = shuffled_X[-100:], shuffled_y[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd7cc3-c146-4547-9ab0-a0ee26fb5173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43eb0eb-7f79-4b97-97b1-aa3e241a1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "batched_train_X = np.array_split(train_X, np.ceil(len(train_X) / batch_size))\n",
    "batched_train_y = np.array_split(train_y, np.ceil(len(train_X) / batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800499a-6cb5-4dd3-a93f-3f8ca6224075",
   "metadata": {},
   "source": [
    "### Training with 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85579c2-0f2c-41fd-a523-9737bf28dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "epochs = 200\n",
    "for _ in tqdm(range(epochs)):\n",
    "    for x_batch,y_batch in zip(batched_train_X,batched_train_y):\n",
    "        loss = train_network(neural_network,x_batch  ,y_batch)\n",
    "    if not _ % 10:\n",
    "        test_preds = sigmoid(neural_network.forward(test_X))  \n",
    "        loss = np.mean(cross_entropy_loss(np.reshape(test_y,(test_y.shape[0], 1)),test_preds))\n",
    "        print(f' train loss at epoch {_} is {loss}, test loss : {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279f37f-82d0-4038-a259-6fd2f79ed73f",
   "metadata": {},
   "source": [
    "## Introducing Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53018fb8-b25c-48b1-8c31-35c619843397",
   "metadata": {},
   "source": [
    "It looks like our model is converging, but how do we know it's actually predicting breast cancer correctly? This is where we introduce  evaluation metrics. For now we will simply focus on accuracy, that is defined as  $$\\frac{TruePositive + TrueNegative}{TruePositive + TrueNegative + FalsePositive + FalseNegative}$$\n",
    "True positives are the ones the model predicts as 1 and the label was 1 True Negatives are elements whethe the model predicts 0 and the label is 0, False Positives are elements where the model predicts 1 and the label is 0 and False Negatives are elements where the model predicts 1 and the label is 0. A model's prediction for this basic case can be defined as $$\n",
    "Pred(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\hat{y} > .5 \\text{ (input layer)} \\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e79c01-0907-48a8-a31b-3b46ade32415",
   "metadata": {},
   "source": [
    "Using this definition we can write a function that calculates the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff08737-adb6-4407-adb7-ebabbcc69d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y,y_hat):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for yi,yhi in zip(y,y_hat):\n",
    "        if yhi > .5:\n",
    "            yhati = 1\n",
    "        else:\n",
    "            yhati = 0\n",
    "        if yi == yhati:\n",
    "            if yi == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                true_negatives += 1\n",
    "        else:\n",
    "            if yhati == 1:\n",
    "                false_positives +=1\n",
    "            else:\n",
    "                false_negatives += 1\n",
    "    return (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd95cb-1951-4052-9f9b-380b62f66707",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = sigmoid(neural_network.forward(test_X))  \n",
    "test_preds = np.reshape(test_preds,(test_y.shape[0],))\n",
    "accuracy(test_y ,test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee24a66-48e1-4351-a4a0-2095ae1e6b87",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This model clearly shows a very nice and healthy accuracy of over 80% on the breast cancer data set. It is possible to do better through things like normalizing the features, but that is outside of the scope of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python alea-jupyter",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
